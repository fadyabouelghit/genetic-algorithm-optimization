"""
Train a PPO agent that replaces the GA by calling the MATLAB SINR evaluation.

Requirements:
    - MATLAB Engine for Python must be installed and reachable on PYTHONPATH.
    - `gymnasium` (or `gym` <=0.26) and `stable-baselines3` must be installed.
    - Quadriga source directory plus this repo must be added to the MATLAB path.

The environment exposes a 5D tuple per flying BS:
    [tx_x, tx_y, tx_height, tx_power, power_status]
and internally calls `SINREvaluation` via the MATLAB engine while preserving
the cached MBS power maps generated by `precompute_mbs_power_maps`.
"""
from __future__ import annotations

import os
import json
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Optional, Sequence, Tuple

import gymnasium as gym
import matlab.engine
import matlab  # type: ignore
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.base_class import BaseAlgorithm
from stable_baselines3.common.monitor import Monitor


@dataclass
class RewardWeights:
    """Weights for the scalar reward shaping."""

    beta: float = 1
    gamma: float = 0
    fbs_weight: float = 0.4
    fbs_exponent: float = 1.0
    epsilon: float = 1e-4


@dataclass
class PPOTrainingConfig:
    """Configuration object for running PPO training."""

    quadriga_path: str
    repo_path: str
    num_fbs: int = 1
    num_users: int = 1000
    sinr_threshold: float = 5.0
    max_episode_steps: int = 25
    total_timesteps: int = 10_000
    verbose: int = 1
    tensorboard_log: Optional[str] = None
    reward_weights: RewardWeights = field(default_factory=RewardWeights)
    ent_coef: float = 0.0
    learning_rate: float = 3e-4
    action_scale: float = 0.05
    max_users: Optional[int] = None
    world_width: Optional[float] = None
    world_height: Optional[float] = None
    num_mbs: Optional[int] = None
    mbs_locations: Optional[Sequence[Tuple[float, float]]] = None


def _matlab_scalar(value: float) -> matlab.double:
    """Helper to make a 1x1 matlab double."""
    return matlab.double([float(value)])


def run_sinr_evaluation(
    eng: matlab.engine.MatlabEngine,
    antenna_fbs,
    antenna_mbs,
    mbs_cache,
    tx_vectors: np.ndarray,
    power_status: np.ndarray | float,
    area_bounds: Tuple[float, float, float, float],
    num_users: int,
    sinr_threshold: float,
    contains_mbs: bool,
    mbs_x,
    mbs_y,
    mbs_height,
    mbs_power,
    num_fbs: Optional[int] = None,
) -> Tuple[int, float, float, int, int, np.ndarray]:
    """Call MATLAB SINREvaluation and return aggregated metrics."""
    tx_vectors = np.asarray(tx_vectors, dtype=float)
    if tx_vectors.ndim == 1:
        tx_vectors = tx_vectors.reshape(1, -1)
    if tx_vectors.shape[1] != 4:
        raise ValueError("tx_vectors must have shape (num_fbs, 4)")
    num_fbs = num_fbs or tx_vectors.shape[0]

    power_status_arr = np.asarray(power_status, dtype=float).reshape(-1)
    if power_status_arr.size == 1:
        power_status_arr = np.repeat(power_status_arr, tx_vectors.shape[0])
    if power_status_arr.size != tx_vectors.shape[0]:
        raise ValueError("power_status must broadcast to num_fbs entries")

    x_min, x_max, y_min, y_max = area_bounds

    power_status_mat = matlab.double([power_status_arr.tolist()])
    tx_x_mat = matlab.double([tx_vectors[:, 0].tolist()])
    tx_y_mat = matlab.double([tx_vectors[:, 1].tolist()])
    tx_height_mat = matlab.double([tx_vectors[:, 2].tolist()])
    tx_power_mat = matlab.double([tx_vectors[:, 3].tolist()])

    num_users_mat = _matlab_scalar(num_users)
    threshold_mat = _matlab_scalar(sinr_threshold)
    no_fbs = _matlab_scalar(num_fbs)

    contains_mbs_mat = matlab.logical([contains_mbs])

    result = eng.SINREvaluation(
        antenna_fbs,
        power_status_mat,
        tx_x_mat,
        tx_y_mat,
        tx_height_mat,
        no_fbs,
        tx_power_mat,
        mbs_x,
        mbs_y,
        mbs_height,
        mbs_power,
        _matlab_scalar(x_min),
        _matlab_scalar(x_max),
        _matlab_scalar(y_min),
        _matlab_scalar(y_max),
        num_users_mat,
        threshold_mat,
        contains_mbs_mat,
        antenna_mbs,
        mbs_cache,
        nargout=7,
    )

    user_positions = np.array(result[0]._data).reshape(result[0].size[::-1]).T
    total_connected = int(result[2])
    total_power = float(result[3])
    avg_rate = float(result[4])
    fbs_connected = int(result[5])
    mbs_connected = int(result[6])

    return total_connected, total_power, avg_rate, fbs_connected, mbs_connected, user_positions


class FlyingBaseStationEnv(gym.Env):
    """Gym environment that optimizes N FBSs using MATLAB SINR."""

    metadata = {"render_modes": []}

    def __init__(
        self,
        quadriga_path: str,
        repo_path: str,
        num_fbs: int = 1,
        num_users: int = 1000,
        max_users: Optional[int] = None,
        sinr_threshold: float = 5.0,
        reward_weights: Optional[RewardWeights] = None,
        max_episode_steps: int = 25,
        action_scale: float = 0.05,
        world_width: Optional[float] = None,
        world_height: Optional[float] = None,
        num_mbs: Optional[int] = None,
        mbs_locations: Optional[Sequence[Tuple[float, float]]] = None,
    ):
        super().__init__()
        self.reward_weights = reward_weights or RewardWeights()
        self.num_fbs = num_fbs
        self.num_users = num_users
        self.max_users = max_users if max_users is not None else num_users
        self.sinr_threshold = sinr_threshold
        self.max_episode_steps = max_episode_steps
        self._action_scale_factor = action_scale
        self._world_width = world_width
        self._world_height = world_height
        self._num_mbs_override = num_mbs
        self._mbs_location_override = mbs_locations

        self._eng = matlab.engine.start_matlab()
        self._eng.addpath(quadriga_path, nargout=0)
        self._eng.addpath(repo_path, nargout=0)

        self._configure_world()

        self.param_lower_single = np.array([0.0, 0.0, 20.0, 7.0, 0.0], dtype=np.float32)
        self.param_upper_single = np.array([self.W, self.H, 150.0, 10.5, 1.0], dtype=np.float32)
        self.param_lower = np.tile(self.param_lower_single, self.num_fbs)
        self.param_upper = np.tile(self.param_upper_single, self.num_fbs)
        self.min_power = 0.0
        self.max_power = float(self.param_upper_single[3]) * self.num_fbs

        self.observation_space = gym.spaces.Box(
            low=self.param_lower,
            high=self.param_upper,
            dtype=np.float32,
        )
        self.action_space = gym.spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(5 * self.num_fbs,),
            dtype=np.float32,
        )

        single_scale = self.param_upper_single[:4] - self.param_lower_single[:4]
        self._action_scale = (
            self._action_scale_factor * np.tile(single_scale, (self.num_fbs, 1))
        ).astype(np.float32)
        self._state = None
        self._steps = 0

    def _configure_world(self):
        """Clone the GA world: antennas, MBS cache, bounds."""
        default_W, default_H = 2000.0, 1500.0
        self.W = float(self._world_width) if self._world_width is not None else default_W
        self.H = float(self._world_height) if self._world_height is not None else default_H
        self.margin = 100.0
        self.isd = 500.0
        if self._num_mbs_override is not None:
            self.num_mbs_requested = int(self._num_mbs_override)
        else:
            self.num_mbs_requested = 1
        if self.num_mbs_requested <= 0:
            raise ValueError("Number of MBSs must be positive")
        self.mbs_height = 25.0
        self.mbs_power = 20.0
        self.ue_height = 1.5
        self.scenario = "3GPP_38.901_UMa_LOS"
        self.mode = "quick"
        cache_dir = os.path.join(os.path.dirname(__file__), "cache_mbs_maps")

        self.antenna_fbs = self._eng.setup_antenna(nargout=1)
        self.antenna_mbs_template = self._eng.setup_antenna(nargout=1)

        xs = ys = None
        if self._mbs_location_override is not None:
            coords = np.asarray(self._mbs_location_override, dtype=float)
            if coords.ndim != 2 or coords.shape[1] != 2:
                raise ValueError("mbs_locations must be an iterable of (x, y) pairs")
            if coords.shape[0] == 0:
                raise ValueError("mbs_locations must contain at least one coordinate pair")
            if (
                self._num_mbs_override is not None
                and coords.shape[0] != self._num_mbs_override
            ):
                raise ValueError("num_mbs override must match length of mbs_locations")
            self.num_mbs_requested = coords.shape[0]
            xs = matlab.double([coords[:, 0].tolist()])
            ys = matlab.double([coords[:, 1].tolist()])

        if xs is None or ys is None:
            xs, ys = self._eng.generate_hex_sites(
                self.W,
                self.H,
                self.isd,
                self.margin,
                self.num_mbs_requested,
                nargout=2,
            )
            
        (
            mbs_params,
            self.antenna_mbs,
            contains_mbs,
            _,
        ) = self._eng.pack_mbs_params(
            xs,
            ys,
            self.mbs_height,
            self.mbs_power,
            self.antenna_mbs_template,
            nargout=4,
        )
        mbs_params = np.array(mbs_params, dtype=float)
        mbs_params[[0, 1], :] = mbs_params[[1, 0], :]


        self.contains_mbs = bool(int(contains_mbs))

        subset = self._eng.struct(
            "xmin",
            0.0,
            "xmax",
            self.W,
            "ymin",
            0.0,
            "ymax",
            self.H,
        )

        self.mbs_cache = self._eng.precompute_mbs_power_maps(
            mbs_params,
            self.antenna_mbs,
            subset,
            self.scenario,
            self.mode,
            self.ue_height,
            cache_dir,
            True,
            nargout=1,
        )

        # Store individual MBS vectors for repeated SINR calls.
        mbs_np = np.array(mbs_params, dtype=float)
        self.mbs_x = matlab.double(mbs_np[0, :].tolist())
        self.mbs_y = matlab.double(mbs_np[1, :].tolist())
        self.mbs_height = matlab.double(mbs_np[2, :].tolist())
        self.mbs_power = matlab.double(mbs_np[3, :].tolist())

        self.area_bounds = (0.0, self.W, 0.0, self.H)

    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        super().reset(seed=seed)
        self._steps = 0
        rand_cont = self.np_random.random((self.num_fbs, 4))
        cont_range = self.param_upper_single[:4] - self.param_lower_single[:4]
        cont_state = self.param_lower_single[:4] + rand_cont * cont_range
        power_state = self.np_random.integers(0, 2, size=(self.num_fbs, 1))
        state = np.concatenate([cont_state, power_state], axis=1).astype(np.float32)
        self._state = state.reshape(-1)
        return self._state.copy(), {}

    def step(self, action: np.ndarray):
        self._steps += 1
        action = np.asarray(np.clip(action, -1.0, 1.0), dtype=np.float32)
        actions = action.reshape(self.num_fbs, 5)
        state = self._state.reshape(self.num_fbs, 5)
        delta = actions[:, :4] * self._action_scale
        state[:, :4] = np.clip(
            state[:, :4] + delta,
            self.param_lower_single[:4],
            self.param_upper_single[:4],
        )
        state[:, 4] = (actions[:, 4] >= 0.0).astype(np.float32)
        self._state = state.astype(np.float32).reshape(-1)

        (
            total_connected,
            total_power,
            avg_rate,
            fbs_conn,
            mbs_conn,
            user_positions,
        ) = run_sinr_evaluation(
            self._eng,
            self.antenna_fbs,
            self.antenna_mbs,
            self.mbs_cache,
            state[:, :4],
            state[:, 4],
            self.area_bounds,
            self.num_users,
            self.sinr_threshold,
            self.contains_mbs,
            self.mbs_x,
            self.mbs_y,
            self.mbs_height,
            self.mbs_power,
            num_fbs=self.num_fbs,
        )

        weights = self.reward_weights
        norm_num_users = np.clip(total_connected / max(self.max_users, 1), 0.0, 1.0)
        power_range = max(self.max_power - self.min_power, weights.epsilon)
        norm_power = np.clip((total_power - self.min_power) / power_range, 0.0, 1.0)
        total_conn_safe = max(total_connected, 1)
        fbs_share = np.clip(fbs_conn / total_conn_safe, 0.0, 1.0)

        base = (norm_num_users + weights.epsilon) ** weights.beta
        base *= ((1 - norm_power) + weights.epsilon) ** weights.gamma
        fbs_term = (fbs_share + weights.epsilon) ** weights.fbs_exponent
        reward = (1 - weights.fbs_weight) * base + weights.fbs_weight * fbs_term

        terminated = total_connected >= self.num_users
        truncated = self._steps >= self.max_episode_steps
        info = {
            "total_connected": total_connected,
            "fbs_connected": fbs_conn,
            "mbs_connected": mbs_conn,
            "total_power": total_power,
            "avg_rate": avg_rate,
            "user_positions": user_positions,
        }

        return self._state.astype(np.float32), reward, terminated, truncated, info

    def render(self):
        return None

    def close(self):
        if self._eng:
            try:
                self._eng.quit()
            except matlab.engine.EngineError:
                pass
            self._eng = None


class PPOTrainer:
    """Utility to configure, train, and persist the PPO agent."""

    def __init__(self, config: PPOTrainingConfig):
        self.config = config
        raw_env = FlyingBaseStationEnv(
            quadriga_path=config.quadriga_path,
            repo_path=config.repo_path,
            num_fbs=config.num_fbs,
            num_users=config.num_users,
            max_users=config.max_users,
            sinr_threshold=config.sinr_threshold,
            reward_weights=config.reward_weights,
            max_episode_steps=config.max_episode_steps,
            action_scale=config.action_scale,
            world_width=config.world_width,
            world_height=config.world_height,
            num_mbs=config.num_mbs,
            mbs_locations=config.mbs_locations,
        )
        monitor_path = os.path.join(config.repo_path, "ppo_logs", "monitor.csv")
        os.makedirs(os.path.dirname(monitor_path), exist_ok=True)
        self.env = Monitor(raw_env, filename=monitor_path, allow_early_resets=True)

        self.model: BaseAlgorithm = PPO(
            "MlpPolicy",
            self.env,
            verbose=config.verbose,
            tensorboard_log=config.tensorboard_log,
            ent_coef=config.ent_coef,
            learning_rate=config.learning_rate,
        )

    def train(self):
        """Run PPO for the configured number of timesteps."""
        self.model.learn(total_timesteps=self.config.total_timesteps)
        return self

    def save(self, output_path: Optional[str] = None):
        """Save the trained model to disk."""
        base_path = (
            Path(output_path)
            if output_path is not None
            else Path(self.config.repo_path) / "ppo_fbs_agent"
        )
        if self.config.num_fbs > 1:
            base_path = base_path.with_name(f"{base_path.name}_nfbs{self.config.num_fbs}")

        path_str = str(base_path)
        self.model.save(path_str)

        metadata = {"reward_weights": asdict(self.config.reward_weights)}
        metadata_path = base_path.parent / f"{base_path.name}_reward_weights.json"
        with metadata_path.open("w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2)

        return path_str

    def close(self):
        """Dispose of the environment / MATLAB engine."""
        if self.env:
            self.env.close()


def main():
    repo_root = os.path.dirname(os.path.abspath(__file__))
    quadriga_src = "/Users/fadya/Documents/MATLAB/quadriga_src"

    config = PPOTrainingConfig(
        quadriga_path=quadriga_src,
        repo_path=repo_root,
        tensorboard_log=os.path.join(repo_root, "ppo_logs"),
    )

    trainer = PPOTrainer(config)
    try:
        trainer.train()
        trainer.save()
    finally:
        trainer.close()


if __name__ == "__main__":
    main()
