"""
Train a PPO agent that replaces the GA by calling the MATLAB SINR evaluation.

Requirements:
    - MATLAB Engine for Python must be installed and reachable on PYTHONPATH.
    - `gymnasium` (or `gym` <=0.26) and `stable-baselines3` must be installed.
    - Quadriga source directory plus this repo must be added to the MATLAB path.

The environment exposes a 4D continuous action/state vector:
    [tx_x, tx_y, tx_height, tx_power]
and internally calls `SINREvaluation` via the MATLAB engine while preserving
the cached MBS power maps generated by `precompute_mbs_power_maps`.
"""
from __future__ import annotations

import os
from dataclasses import dataclass, field
from typing import Optional, Tuple

import gymnasium as gym
import matlab.engine
import matlab  # type: ignore
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.base_class import BaseAlgorithm
from stable_baselines3.common.monitor import Monitor


@dataclass
class RewardWeights:
    """Weights for the scalar reward shaping."""

    beta_connected: float = 1.0
    gamma_power: float = 0
    delta_avg_rate: float = 0


@dataclass
class PPOTrainingConfig:
    """Configuration object for running PPO training."""

    quadriga_path: str
    repo_path: str
    num_users: int = 1000
    sinr_threshold: float = 5.0
    max_episode_steps: int = 25
    total_timesteps: int = 10_000
    verbose: int = 1
    tensorboard_log: Optional[str] = None
    reward_weights: RewardWeights = field(default_factory=RewardWeights)
    ent_coef: float = 0.0
    learning_rate: float = 3e-4
    action_scale: float = 0.05


def _matlab_scalar(value: float) -> matlab.double:
    """Helper to make a 1x1 matlab double."""
    return matlab.double([float(value)])


def run_sinr_evaluation(
    eng: matlab.engine.MatlabEngine,
    antenna_fbs,
    antenna_mbs,
    mbs_cache,
    tx_vector: np.ndarray,
    power_status: float,
    area_bounds: Tuple[float, float, float, float],
    num_users: int,
    sinr_threshold: float,
    contains_mbs: bool,
    mbs_x,
    mbs_y,
    mbs_height,
    mbs_power,
) -> Tuple[int, float, float, int, int, np.ndarray]:
    """Call MATLAB SINREvaluation and return aggregated metrics."""
    tx_x, tx_y, tx_height, tx_power = map(float, tx_vector)
    x_min, x_max, y_min, y_max = area_bounds

    power_status_mat = matlab.double([float(power_status)])
    tx_x_mat = matlab.double([tx_x])
    tx_y_mat = matlab.double([tx_y])
    tx_height_mat = matlab.double([tx_height])
    tx_power_mat = matlab.double([tx_power])

    num_users_mat = _matlab_scalar(num_users)
    threshold_mat = _matlab_scalar(sinr_threshold)
    no_fbs = _matlab_scalar(1)

    contains_mbs_mat = matlab.logical([contains_mbs])

    result = eng.SINREvaluation(
        antenna_fbs,
        power_status_mat,
        tx_x_mat,
        tx_y_mat,
        tx_height_mat,
        no_fbs,
        tx_power_mat,
        mbs_x,
        mbs_y,
        mbs_height,
        mbs_power,
        _matlab_scalar(x_min),
        _matlab_scalar(x_max),
        _matlab_scalar(y_min),
        _matlab_scalar(y_max),
        num_users_mat,
        threshold_mat,
        contains_mbs_mat,
        antenna_mbs,
        mbs_cache,
        nargout=7,
    )

    user_positions = np.array(result[0]._data).reshape(result[0].size[::-1]).T
    total_connected = int(result[2])
    total_power = float(result[3])
    avg_rate = float(result[4])
    fbs_connected = int(result[5])
    mbs_connected = int(result[6])

    return total_connected, total_power, avg_rate, fbs_connected, mbs_connected, user_positions


class FlyingBaseStationEnv(gym.Env):
    """Gym environment that optimizes a single FBS using MATLAB SINR."""

    metadata = {"render_modes": []}

    def __init__(
        self,
        quadriga_path: str,
        repo_path: str,
        num_users: int = 1000,
        sinr_threshold: float = 5.0,
        reward_weights: Optional[RewardWeights] = None,
        max_episode_steps: int = 25,
        action_scale: float = 0.05,
    ):
        super().__init__()
        self.reward_weights = reward_weights or RewardWeights()
        self.num_users = num_users
        self.sinr_threshold = sinr_threshold
        self.max_episode_steps = max_episode_steps
        self._action_scale_factor = action_scale

        self._eng = matlab.engine.start_matlab()
        self._eng.addpath(quadriga_path, nargout=0)
        self._eng.addpath(repo_path, nargout=0)

        self._configure_world()

        self.param_lower = np.array([0.0, 0.0, 20.0, 7.0, 0.0], dtype=np.float32)
        self.param_upper = np.array([self.W, self.H, 150.0, 10.5, 1.0], dtype=np.float32)

        self.observation_space = gym.spaces.Box(
            low=self.param_lower,
            high=self.param_upper,
            dtype=np.float32,
        )
        self.action_space = gym.spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(5,),
            dtype=np.float32,
        )

        self._action_scale = self._action_scale_factor * (self.param_upper[:4] - self.param_lower[:4])
        self._state = None
        self._steps = 0

    def _configure_world(self):
        """Clone the GA world: antennas, MBS cache, bounds."""
        self.W, self.H = 2000.0, 1500.0
        self.margin = 100.0
        self.isd = 500.0
        self.num_mbs_requested = 1
        self.mbs_height = 25.0
        self.mbs_power = 20.0
        self.ue_height = 1.5
        self.scenario = "3GPP_38.901_UMa_LOS"
        self.mode = "quick"
        cache_dir = os.path.join(os.path.dirname(__file__), "cache_mbs_maps")

        self.antenna_fbs = self._eng.setup_antenna(nargout=1)
        self.antenna_mbs_template = self._eng.setup_antenna(nargout=1)

        xs, ys = self._eng.generate_hex_sites(
            self.W,
            self.H,
            self.isd,
            self.margin,
            self.num_mbs_requested,
            nargout=2,
        )

        (
            mbs_params,
            self.antenna_mbs,
            contains_mbs,
            _,
        ) = self._eng.pack_mbs_params(
            xs,
            ys,
            self.mbs_height,
            self.mbs_power,
            self.antenna_mbs_template,
            nargout=4,
        )
        mbs_params = np.array(mbs_params, dtype=float)
        mbs_params[[0, 1], :] = mbs_params[[1, 0], :]


        self.contains_mbs = bool(int(contains_mbs))

        subset = self._eng.struct(
            "xmin",
            0.0,
            "xmax",
            self.W,
            "ymin",
            0.0,
            "ymax",
            self.H,
        )

        self.mbs_cache = self._eng.precompute_mbs_power_maps(
            mbs_params,
            self.antenna_mbs,
            subset,
            self.scenario,
            self.mode,
            self.ue_height,
            cache_dir,
            nargout=1,
        )

        # Store individual MBS vectors for repeated SINR calls.
        mbs_np = np.array(mbs_params, dtype=float)
        self.mbs_x = matlab.double(mbs_np[0, :].tolist())
        self.mbs_y = matlab.double(mbs_np[1, :].tolist())
        self.mbs_height = matlab.double(mbs_np[2, :].tolist())
        self.mbs_power = matlab.double(mbs_np[3, :].tolist())

        self.area_bounds = (0.0, self.W, 0.0, self.H)

    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
        super().reset(seed=seed)
        self._steps = 0
        rand_cont = self.np_random.random(4)
        cont_state = self.param_lower[:4] + rand_cont * (self.param_upper[:4] - self.param_lower[:4])
        power_state = float(self.np_random.integers(0, 2))
        self._state = np.concatenate([cont_state, [power_state]]).astype(np.float32)
        return self._state.copy(), {}

    def step(self, action: np.ndarray):
        self._steps += 1
        action = np.clip(action, -1.0, 1.0)
        delta = action[:4] * self._action_scale
        self._state[:4] = np.clip(
            self._state[:4] + delta,
            self.param_lower[:4],
            self.param_upper[:4],
        )
        self._state[4] = float(action[4] >= 0.0)

        (
            total_connected,
            total_power,
            avg_rate,
            fbs_conn,
            mbs_conn,
            user_positions,
        ) = run_sinr_evaluation(
            self._eng,
            self.antenna_fbs,
            self.antenna_mbs,
            self.mbs_cache,
            self._state[:4],
            self._state[4],
            self.area_bounds,
            self.num_users,
            self.sinr_threshold,
            self.contains_mbs,
            self.mbs_x,
            self.mbs_y,
            self.mbs_height,
            self.mbs_power,
        )

        reward = (
            self.reward_weights.beta_connected * total_connected
            - self.reward_weights.gamma_power * total_power
            + self.reward_weights.delta_avg_rate * avg_rate
        )

        terminated = total_connected >= self.num_users
        truncated = self._steps >= self.max_episode_steps
        info = {
            "total_connected": total_connected,
            "fbs_connected": fbs_conn,
            "mbs_connected": mbs_conn,
            "total_power": total_power,
            "avg_rate": avg_rate,
            "user_positions": user_positions,
        }

        return self._state.astype(np.float32), reward, terminated, truncated, info

    def render(self):
        return None

    def close(self):
        if self._eng:
            try:
                self._eng.quit()
            except matlab.engine.EngineError:
                pass
            self._eng = None


class PPOTrainer:
    """Utility to configure, train, and persist the PPO agent."""

    def __init__(self, config: PPOTrainingConfig):
        self.config = config
        raw_env = FlyingBaseStationEnv(
            quadriga_path=config.quadriga_path,
            repo_path=config.repo_path,
            num_users=config.num_users,
            sinr_threshold=config.sinr_threshold,
            reward_weights=config.reward_weights,
            max_episode_steps=config.max_episode_steps,
            action_scale=config.action_scale,
        )
        monitor_path = os.path.join(config.repo_path, "ppo_logs", "monitor.csv")
        os.makedirs(os.path.dirname(monitor_path), exist_ok=True)
        self.env = Monitor(raw_env, filename=monitor_path, allow_early_resets=True)

        self.model: BaseAlgorithm = PPO(
            "MlpPolicy",
            self.env,
            verbose=config.verbose,
            tensorboard_log=config.tensorboard_log,
            ent_coef=config.ent_coef,
            learning_rate=config.learning_rate,
        )

    def train(self):
        """Run PPO for the configured number of timesteps."""
        self.model.learn(total_timesteps=self.config.total_timesteps)
        return self

    def save(self, output_path: Optional[str] = None):
        """Save the trained model to disk."""
        path = output_path or os.path.join(self.config.repo_path, "ppo_fbs_agent")
        self.model.save(path)
        return path

    def close(self):
        """Dispose of the environment / MATLAB engine."""
        if self.env:
            self.env.close()


def main():
    repo_root = os.path.dirname(os.path.abspath(__file__))
    quadriga_src = "/Users/fadya/Documents/MATLAB/quadriga_src"

    config = PPOTrainingConfig(
        quadriga_path=quadriga_src,
        repo_path=repo_root,
        tensorboard_log=os.path.join(repo_root, "ppo_logs"),
    )

    trainer = PPOTrainer(config)
    try:
        trainer.train()
        trainer.save()
    finally:
        trainer.close()


if __name__ == "__main__":
    main()
